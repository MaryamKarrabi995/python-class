{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:42.398542\n",
      "Measures:\n",
      "HS: 0.9740595825525812\n",
      "CS: 0.9740795372957063\n",
      "VM: 0.9740695598219457\n",
      "AMI: 0.9740459035565486\n",
      "ARI: 0.9851015157380054\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "########\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType, DataType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest, desc, asc, row_number, avg, mean, least, struct, lit, sequence, sum\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"12g\").\\\n",
    "config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\", \"14g\").appName(\"NPIR_Parallel\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "x1,y1 = make_blobs(n_samples=2000, centers=3, n_features=2,\n",
    "                random_state=1)\n",
    "data = pd.DataFrame(x1)\n",
    "data['label'] = y1\n",
    "data.head()\n",
    "\n",
    "data = shuffle(data)\n",
    "data.head()\n",
    "\n",
    "data.to_csv('blobs.csv', index=False)\n",
    "\n",
    "#read csv\n",
    "data_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('blobs.csv')\n",
    "# data_spark_df.show()\n",
    "\n",
    "data_spark_df = data_spark_df.select('0', '1')\n",
    "\n",
    "new_name = ['first', 'second']\n",
    "data_spark_rdd = data_spark_df.toDF(*new_name).rdd.filter(lambda x:x)\n",
    "data_spark_df = data_spark_df.toDF(*new_name)\n",
    "# data_spark_df.show()\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"False\")\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "\n",
    "points = data_spark_df.count()\n",
    "# points\n",
    "\n",
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()\n",
    "chunk = 400\n",
    "\n",
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_spark = data_spark_df.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# data_spark.show()\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "leaderheadr = ['chunkLabel', 'old label', 'count']\n",
    "leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "leaderheadr = tuple(leaderheadr)\n",
    "leaderheadr\n",
    "\n",
    "labelsheader = ('chunkLabel', 'label')\n",
    "labelsheader\n",
    "\n",
    "\n",
    "def Euclidean_distance(a,b,c,d):\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    d = float(d)\n",
    "    array1 = np.array([a,b])\n",
    "    array2 = np.array([c,d])\n",
    "    dist = np.linalg.norm(array1-array2)\n",
    "    dist = dist.item()\n",
    "    return dist\n",
    "\n",
    "Euclidean_distance_spark = udf(lambda x,y,z,t: Euclidean_distance(x,y,z,t), FloatType())\n",
    "sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")\n",
    "\n",
    "start = timer()\n",
    "\n",
    "\n",
    "labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "labels = labels.na.drop()\n",
    "\n",
    "leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "leaders = leaders.na.drop()\n",
    "\n",
    "ii = 0\n",
    "for z in range(0, points, chunk):\n",
    "    j = z + chunk\n",
    "    data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "    data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "    data = data.astype(float)\n",
    "    from NPIR import NPIR\n",
    "    label = NPIR(data.values,k,IR,i)\n",
    "    data['labels'] = label\n",
    "    \n",
    "    # Adding to pyspard label\n",
    "    chunklabel = np.full(len(label), ii).tolist()\n",
    "    labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "    labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "    labels = unionAll(labels, labelsDF)\n",
    "\n",
    "    leader = []\n",
    "    f = list(Cs(label))\n",
    "    f.sort()\n",
    "    for i in f:\n",
    "        leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:,:-1].T])\n",
    "    del data\n",
    "    del NPIR\n",
    "    \n",
    "    # Adding to pyspark leaders\n",
    "    for x in range(len(leader)):\n",
    "        x1 = [ii, x, Cs(label)[x]]\n",
    "        x1.extend(leader[x])\n",
    "        leader[x] = x1\n",
    "    leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "    leaders = unionAll(leaders, leaderDF)\n",
    "    ii += 1\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "# # Load and parse the data\n",
    "# data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "# parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "parsedData = leaders.select(['1', '2']).rdd.map(list)\n",
    "\n",
    "# start = timer()\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "# start = timer()\n",
    "\n",
    "spark_cluster_centroid = sqlContext.createDataFrame(([c.tolist() for c in clusters.centers]),\\\n",
    "                                                    ['cent_x', 'cent_y'])\n",
    "# spark_cluster_centroid.show()\n",
    "spark_cluster_centroid = spark_cluster_centroid.withColumn('defined_cluster', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# spark_cluster_centroid.show()\n",
    "data_cent = 0\n",
    "for i in range(k):\n",
    "    u = [ str(i)+'x',str(i)+'y']\n",
    "    centroids = spark_cluster_centroid.filter(col('defined_cluster') == str(i)).\\\n",
    "    drop('defined_cluster').toDF(*u)\n",
    "#     centroids.show()\n",
    "    if i == 0:\n",
    "        data_cent = data_spark_df.join(broadcast(centroids))        \n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn(str(i),Euclidean_distance_spark(data_cent.columns[0],\\\n",
    "                            data_cent.columns[1],data_cent.columns[i+2],data_cent.columns[i+3]))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop(data_cent.columns[i+2]).drop(data_cent.columns[i+3])\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn('mindist',col(str(i)))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn('mindist1',least(data_cent.columns[i+2], col('mindist')))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop('mindist')\n",
    "        data_cent = data_cent.withColumnRenamed('mindist1','mindist')\n",
    "#         data_cent\n",
    "    elif i > 0:\n",
    "        data_cent = data_cent.join(broadcast(centroids))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn(str(i),Euclidean_distance_spark(data_cent.columns[0],\\\n",
    "                                    data_cent.columns[1], data_cent.columns[i+3],data_cent.columns[i+4]))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop(u[0]).drop(u[1])\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn('mindist1',least(data_cent.columns[i+3], col('mindist')))#4\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop('mindist')\n",
    "        data_cent = data_cent.withColumnRenamed('mindist1','mindist')\n",
    "        \n",
    "data_cent = data_cent.drop('mindist')\n",
    "\n",
    "# data_cent.show()\n",
    "\n",
    "def Min_COl( *row_list):\n",
    "    ind = row_list.index(min(*row_list))\n",
    "    return int(ind)\n",
    "find_min_val_name = udf(Min_COl, IntegerType())\n",
    "\n",
    "data_cent = data_cent.withColumn('defined_cluster', find_min_val_name(*data_cent.columns[2:3+k]))\n",
    "#data_cent.show()\n",
    "data_cent = data_cent.select('first','second','defined_cluster')\n",
    "\n",
    "d = data_cent.toPandas()\n",
    "\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "data = pd.read_csv('blobs.csv')\n",
    "# data.head()\n",
    "\n",
    "data = data[['0','1','label']]\n",
    "\n",
    "data['new'] = d['defined_cluster'].astype('int8')\n",
    "data.head()\n",
    "\n",
    "y = data['label']\n",
    "labelsPred = data['new']\n",
    "\n",
    "x = data[['0','1']].values\n",
    "x.shape\n",
    "\n",
    "#printing results\n",
    "#print('labels:')\n",
    "# print(labelsPred)\n",
    "\n",
    "# tEnd = datetime.datetime.now()\n",
    "# print('Time: ' + str(tEnd - tStart))\n",
    "print('Measures:')\n",
    "print('HS: ' + str(metrics.homogeneity_score(y,labelsPred)))\n",
    "print('CS: ' + str(metrics.completeness_score(y,labelsPred)))\n",
    "print('VM: ' + str(metrics.v_measure_score(y,labelsPred)))\n",
    "print('AMI: ' + str(metrics.adjusted_mutual_info_score(y,labelsPred)))\n",
    "print('ARI: ' + str(metrics.adjusted_rand_score(y,labelsPred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
