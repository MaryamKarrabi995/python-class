{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "########\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType, DataType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest, desc, asc, row_number, avg, mean, least, struct, lit, sequence, sum\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"12g\").\\\n",
    "config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\", \"14g\").appName(\"NPIR_Parallel\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.794152</td>\n",
       "      <td>2.104951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.151552</td>\n",
       "      <td>-4.812864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-11.441826</td>\n",
       "      <td>-4.457814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.767618</td>\n",
       "      <td>-3.191337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.536556</td>\n",
       "      <td>-8.401863</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1  label\n",
       "0  -0.794152  2.104951      0\n",
       "1  -9.151552 -4.812864      1\n",
       "2 -11.441826 -4.457814      1\n",
       "3  -9.767618 -3.191337      1\n",
       "4  -4.536556 -8.401863      2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = make_blobs(n_samples=100, centers=3, n_features=2,\n",
    "                random_state=1)\n",
    "data = pd.DataFrame(x1)\n",
    "data['label'] = y1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-1.782450</td>\n",
       "      <td>3.470720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.000242</td>\n",
       "      <td>5.148534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-6.195996</td>\n",
       "      <td>-7.402816</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.526016</td>\n",
       "      <td>3.009994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-1.617346</td>\n",
       "      <td>4.989305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1  label\n",
       "49 -1.782450  3.470720      0\n",
       "74  0.000242  5.148534      0\n",
       "76 -6.195996 -7.402816      2\n",
       "47  0.526016  3.009994      0\n",
       "58 -1.617346  4.989305      0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('blobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|                   0|                  1|label|\n",
      "+--------------------+-------------------+-----+\n",
      "| -1.7824501314671677| 3.4707204345840927|    0|\n",
      "|0.000242271161351...|  5.148534029420497|    0|\n",
      "|  -6.195996026651871| -7.402816464759037|    2|\n",
      "|  0.5260155005846419|  3.009993533355024|    0|\n",
      "| -1.6173461592329268| 4.9893050825589835|    0|\n",
      "|  -6.816083022269968| -8.449869256994909|    2|\n",
      "| -1.9819771099620271|  4.022435514174746|    0|\n",
      "|  -9.204905637733754|-4.5768792770429965|    1|\n",
      "| -7.2013269275537715| -8.272282292398854|    2|\n",
      "| -1.9274479855745354| 4.9368453355813475|    0|\n",
      "|  -7.374998957175799|-10.588065868731183|    2|\n",
      "|-0.19745196890354544| 2.3463491593455075|    0|\n",
      "|  -0.757969185355724|  4.908984207745029|    0|\n",
      "|  -6.569670859679778| -8.327931264366546|    2|\n",
      "| -1.6087521511724905|  3.769494222273808|    0|\n",
      "| -11.227770639320063| -3.402811051386989|    1|\n",
      "|  -8.798794623751593|-3.7681921298792607|    1|\n",
      "| -1.5394009534668904|  5.023692978550581|    0|\n",
      "|  -4.874182454688006|-10.049589027515138|    2|\n",
      "|  -5.733425071070147| -8.440535968100065|    2|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "data_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('blobs.csv')\n",
    "data_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, 0: string, 1: string, label: string]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: string (nullable = true)\n",
      " |-- 1: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark_df = data_spark_df.select('0', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|               first|             second|\n",
      "+--------------------+-------------------+\n",
      "| -1.7824501314671677| 3.4707204345840927|\n",
      "|0.000242271161351...|  5.148534029420497|\n",
      "|  -6.195996026651871| -7.402816464759037|\n",
      "|  0.5260155005846419|  3.009993533355024|\n",
      "| -1.6173461592329268| 4.9893050825589835|\n",
      "|  -6.816083022269968| -8.449869256994909|\n",
      "| -1.9819771099620271|  4.022435514174746|\n",
      "|  -9.204905637733754|-4.5768792770429965|\n",
      "| -7.2013269275537715| -8.272282292398854|\n",
      "| -1.9274479855745354| 4.9368453355813475|\n",
      "|  -7.374998957175799|-10.588065868731183|\n",
      "|-0.19745196890354544| 2.3463491593455075|\n",
      "|  -0.757969185355724|  4.908984207745029|\n",
      "|  -6.569670859679778| -8.327931264366546|\n",
      "| -1.6087521511724905|  3.769494222273808|\n",
      "| -11.227770639320063| -3.402811051386989|\n",
      "|  -8.798794623751593|-3.7681921298792607|\n",
      "| -1.5394009534668904|  5.023692978550581|\n",
      "|  -4.874182454688006|-10.049589027515138|\n",
      "|  -5.733425071070147| -8.440535968100065|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_name = ['first', 'second']\n",
    "data_spark_rdd = data_spark_df.toDF(*new_name).rdd.filter(lambda x:x)\n",
    "data_spark_df = data_spark_df.toDF(*new_name)\n",
    "data_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"False\")\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = data_spark_df.count()\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()\n",
    "chunk = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------------+\n",
      "|               first|             second|index_column_name|\n",
      "+--------------------+-------------------+-----------------+\n",
      "| -1.7824501314671677| 3.4707204345840927|                0|\n",
      "|0.000242271161351...|  5.148534029420497|                1|\n",
      "|  -6.195996026651871| -7.402816464759037|                2|\n",
      "|  0.5260155005846419|  3.009993533355024|                3|\n",
      "| -1.6173461592329268| 4.9893050825589835|                4|\n",
      "|  -6.816083022269968| -8.449869256994909|                5|\n",
      "| -1.9819771099620271|  4.022435514174746|                6|\n",
      "|  -9.204905637733754|-4.5768792770429965|                7|\n",
      "| -7.2013269275537715| -8.272282292398854|                8|\n",
      "| -1.9274479855745354| 4.9368453355813475|                9|\n",
      "|  -7.374998957175799|-10.588065868731183|               10|\n",
      "|-0.19745196890354544| 2.3463491593455075|               11|\n",
      "|  -0.757969185355724|  4.908984207745029|               12|\n",
      "|  -6.569670859679778| -8.327931264366546|               13|\n",
      "| -1.6087521511724905|  3.769494222273808|               14|\n",
      "| -11.227770639320063| -3.402811051386989|               15|\n",
      "|  -8.798794623751593|-3.7681921298792607|               16|\n",
      "| -1.5394009534668904|  5.023692978550581|               17|\n",
      "|  -4.874182454688006|-10.049589027515138|               18|\n",
      "|  -5.733425071070147| -8.440535968100065|               19|\n",
      "+--------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_spark = data_spark_df.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "data_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'old label', 'count', '1', '2')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderheadr = ['chunkLabel', 'old label', 'count']\n",
    "leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "leaderheadr = tuple(leaderheadr)\n",
    "leaderheadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'label')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsheader = ('chunkLabel', 'label')\n",
    "labelsheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:00.903166\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "\n",
    "labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "labels = labels.na.drop()\n",
    "\n",
    "leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "leaders = leaders.na.drop()\n",
    "\n",
    "ii = 0\n",
    "for z in range(0, points, chunk):\n",
    "    j = z + chunk\n",
    "    data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "    data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "    data = data.astype(float)\n",
    "    from NPIR import NPIR\n",
    "    label = NPIR(data.values,k,IR,i)\n",
    "    data['labels'] = label\n",
    "    \n",
    "    # Adding to pyspard label\n",
    "    chunklabel = np.full(len(label), ii).tolist()\n",
    "    labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "    labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "    labels = unionAll(labels, labelsDF)\n",
    "\n",
    "    leader = []\n",
    "    f = list(Cs(label))\n",
    "    f.sort()\n",
    "    for i in f:\n",
    "        leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:,:-1].T])\n",
    "    del data\n",
    "    del NPIR\n",
    "    \n",
    "    # Adding to pyspark leaders\n",
    "    for x in range(len(leader)):\n",
    "        x1 = [ii, x, Cs(label)[x]]\n",
    "        x1.extend(leader[x])\n",
    "        leader[x] = x1\n",
    "    leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "    leaders = unionAll(leaders, leaderDF)\n",
    "    ii += 1\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and parse the data\n",
    "# data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "# parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "parsedData = leaders.select(['1', '2']).rdd.map(list)\n",
    "\n",
    "start = timer()\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance(a,b,c,d):\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    d = float(d)\n",
    "    array1 = np.array([a,b])\n",
    "    array2 = np.array([c,d])\n",
    "    dist = np.linalg.norm(array1-array2)\n",
    "    dist = dist.item()\n",
    "    return dist\n",
    "\n",
    "Euclidean_distance_spark = udf(lambda x,y,z,t: Euclidean_distance(x,y,z,t), FloatType())\n",
    "sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "spark_cluster_centroid = sqlContext.createDataFrame(([c.tolist() for c in clusters.centers]),\\\n",
    "                                                    ['cent_x', 'cent_y'])\n",
    "# spark_cluster_centroid.show()\n",
    "spark_cluster_centroid = spark_cluster_centroid.withColumn('defined_cluster', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# spark_cluster_centroid.show()\n",
    "data_cent = 0\n",
    "for i in range(k):\n",
    "    u = [ str(i)+'x',str(i)+'y']\n",
    "    centroids = spark_cluster_centroid.filter(col('defined_cluster') == str(i)).\\\n",
    "    drop('defined_cluster').toDF(*u)\n",
    "#     centroids.show()\n",
    "    if i == 0:\n",
    "        data_cent = data_spark_df.join(broadcast(centroids))        \n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn(str(i),Euclidean_distance_spark(data_cent.columns[0],\\\n",
    "                            data_cent.columns[1],data_cent.columns[i+2],data_cent.columns[i+3]))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop(data_cent.columns[i+2]).drop(data_cent.columns[i+3])\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn('mindist',col(str(i)))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn('mindist1',least(data_cent.columns[i+2], col('mindist')))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop('mindist')\n",
    "        data_cent = data_cent.withColumnRenamed('mindist1','mindist')\n",
    "#         data_cent\n",
    "    elif i > 0:\n",
    "        data_cent = data_cent.join(broadcast(centroids))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn(str(i),Euclidean_distance_spark(data_cent.columns[0],\\\n",
    "                                    data_cent.columns[1], data_cent.columns[i+3],data_cent.columns[i+4]))\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop(u[0]).drop(u[1])\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.withColumn('mindist1',least(data_cent.columns[i+3], col('mindist')))#4\n",
    "#         data_cent.show()\n",
    "        data_cent = data_cent.drop('mindist')\n",
    "        data_cent = data_cent.withColumnRenamed('mindist1','mindist')\n",
    "        \n",
    "data_cent = data_cent.drop('mindist')\n",
    "\n",
    "# data_cent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Min_COl( *row_list):\n",
    "    ind = row_list.index(min(*row_list))\n",
    "    return int(ind)\n",
    "find_min_val_name = udf(Min_COl, IntegerType())\n",
    "\n",
    "data_cent = data_cent.withColumn('defined_cluster', find_min_val_name(*data_cent.columns[2:3+k]))\n",
    "#data_cent.show()\n",
    "data_cent = data_cent.select('first','second','defined_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_cent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_cent.toPandas()\n",
    "\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('blobs.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['0','1','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new'] = d['defined_cluster'].astype('int8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label']\n",
    "labelsPred = data['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['0','1']].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing results\n",
    "#print('labels:')\n",
    "# print(labelsPred)\n",
    "\n",
    "# tEnd = datetime.datetime.now()\n",
    "# print('Time: ' + str(tEnd - tStart))\n",
    "print('Measures:')\n",
    "print('HS: ' + str(metrics.homogeneity_score(y,labelsPred)))\n",
    "print('CS: ' + str(metrics.completeness_score(y,labelsPred)))\n",
    "print('VM: ' + str(metrics.v_measure_score(y,labelsPred)))\n",
    "print('AMI: ' + str(metrics.adjusted_mutual_info_score(y,labelsPred)))\n",
    "print('ARI: ' + str(metrics.adjusted_rand_score(y,labelsPred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle, islice \n",
    "\n",
    "fig = plt.figure()      \n",
    "colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "                                    '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "plt.scatter(x[:, 0], x[:, 1], s=10, color=colors[labelsPred.tolist()])\n",
    "plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
