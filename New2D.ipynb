{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "########\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType, DataType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest, desc, asc, row_number, avg, mean, least, struct, lit, sequence, sum, concat\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"12g\").\\\n",
    "config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\", \"25g\").appName(\"NPIR_Parallel\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "x1,y1 = make_blobs(n_samples=1000, centers=3, n_features=2,\n",
    "                random_state=1)\n",
    "data = pd.DataFrame(x1)\n",
    "data['label'] = y1\n",
    "data.head()\n",
    "\n",
    "data = shuffle(data)\n",
    "data.head()\n",
    "data.to_csv('blobs2.csv', index=False)\n",
    "#read csv\n",
    "data_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('blobs2.csv')\n",
    "# data_spark_df.show()\n",
    "data_spark_df = data_spark_df.select(data_spark_df.columns[:-1])\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"False\")\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "\n",
    "points = data_spark_df.count()\n",
    "points\n",
    "\n",
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()\n",
    "chunk = 250\n",
    "\n",
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_spark = data_spark_df.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# data_spark.show()\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "leaderheadr = ['chunkLabel', 'old label'\n",
    "               #, 'count'\n",
    "              ]\n",
    "leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "leaderheadr = tuple(leaderheadr)\n",
    "leaderheadr\n",
    "\n",
    "labelsheader = ('chunkLabel', 'label')\n",
    "labelsheader\n",
    "\n",
    "start = timer()\n",
    "\n",
    "\n",
    "labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "labels = labels.na.drop()\n",
    "\n",
    "leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "leaders = leaders.na.drop()\n",
    "\n",
    "ii = 0\n",
    "for z in range(0, points, chunk):\n",
    "    j = z + chunk\n",
    "    data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "    data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "    data = data.astype(float)\n",
    "    from NPIR import NPIR\n",
    "    label = NPIR(data.values,k,IR,i)\n",
    "    data['labels'] = label\n",
    "    \n",
    "    # Adding to pyspard label\n",
    "    chunklabel = np.full(len(label), ii).tolist()\n",
    "    labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "    labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "    labels = unionAll(labels, labelsDF)\n",
    "\n",
    "    leader = []\n",
    "    f = list(Cs(label))\n",
    "    f.sort()\n",
    "    for i in f:\n",
    "        leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:,:-1].T])\n",
    "    del data\n",
    "    del NPIR\n",
    "    \n",
    "    # Adding to pyspark leaders\n",
    "    for x in range(len(leader)):\n",
    "        x1 = [ii, x]\n",
    "        x1.extend(leader[x])\n",
    "        leader[x] = x1\n",
    "    leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "    leaders = unionAll(leaders, leaderDF)\n",
    "    ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and parse the data\n",
    "# data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "# parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "# parsedData = leaders.select(leaders.columns[2:]).rdd.map(list)\n",
    "parsedData = leaders.select(['1', '2']).rdd.map(list)\n",
    "# start = timer()\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\", timedelta(seconds = end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaree1(c,u,f,g):\n",
    "    c = float(c)\n",
    "    u = float(u)\n",
    "    f = float(f)\n",
    "    g = float(g)\n",
    "    array1 = np.array([c,u])\n",
    "    array2 = np.array([f,g])\n",
    "    dist = np.linalg.norm(array1-array2)\n",
    "    dist = dist.item()\n",
    "    return dist\n",
    "\n",
    "squaree_spark1 = udf(lambda x,y,z,r: squaree1(x,y,z,r), FloatType())\n",
    "sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")\n",
    "\n",
    "# start = timer()\n",
    "leaderlabe = sqlContext.createDataFrame(clusters.predict(parsedData).map(lambda x: (x, )),\\\n",
    "            ['defined_cluster']).withColumn('index1', row_number().\\\n",
    "            over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# leaderlabe.show()\n",
    "\n",
    "\n",
    "spark_cluster_centroid = parsedData.toDF().withColumn('index', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "spark_cluster_centroid = (spark_cluster_centroid\n",
    "    .join(leaderlabe, (col('index') == col('index1')), \"left\")).\\\n",
    "drop('index').drop('index1')\n",
    "\n",
    "final_data = spark_cluster_centroid.withColumn('index', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "leaders1 = leaders.withColumnRenamed('old label', 'old_label')\n",
    "leaders1.registerTempTable(\"leaders1\")\n",
    "leaders1 = sqlContext.sql(\"SELECT CONCAT(chunkLabel, old_label) as chunkOldLabel FROM leaders1\")\n",
    "\n",
    "leaders1 = leaders1.withColumn('index_column_name', row_number().\\\n",
    "                    over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "lead_final = (final_data\n",
    "    .join(leaders1, (col('index')) == col('index_column_name'), \"left\")).\\\n",
    "drop('index_column_name').drop('index')\n",
    "\n",
    "for c in final_data.columns[:-2]:\n",
    "    lead_final = lead_final.drop(c)\n",
    "    \n",
    "labels.registerTempTable(\"labels\")\n",
    "labels1 = sqlContext.sql(\"SELECT CONCAT(chunkLabel, label) as chunkLabel FROM labels\")\n",
    "lead_final = (final_data\n",
    "    .join(leaders1, (col('index')) == col('index_column_name'), \"left\")).\\\n",
    "drop('index_column_name').drop('index').drop('_1').drop('_2').drop('_3')\n",
    "\n",
    "labels1 = labels1.withColumn('index', row_number().\\\n",
    "                    over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "label_lead_final = (labels1.join(lead_final, (col('chunkLabel') == col('chunkOldLabel')),\"left\"))\n",
    "\n",
    "label_lead_final = label_lead_final.orderBy('index',ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = label_lead_final.toPandas()\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "data = pd.read_csv('blobs2.csv')\n",
    "data.head()\n",
    "\n",
    "data = data[['0','1', 'label']]\n",
    "\n",
    "data['new'] = d['defined_cluster'].astype('int8')\n",
    "# data['new'] = NPIR(pd.read_csv('blobs.csv', header=None).iloc[:,:2].values,k,IR,i)\n",
    "data.head()\n",
    "\n",
    "y = data['label']\n",
    "labelsPred = data['new']\n",
    "# list(labelsPred)\n",
    "\n",
    "x = data[['0','1']].values\n",
    "x.shape\n",
    "\n",
    "#printing results\n",
    "print('labels:')\n",
    "# print(labelsPred)\n",
    "\n",
    "# tEnd = datetime.datetime.now()\n",
    "# print('Time: ' + str(tEnd - tStart))\n",
    "print('Measures:')\n",
    "print('HS: ' + str(metrics.homogeneity_score(y,labelsPred)))\n",
    "print('CS: ' + str(metrics.completeness_score(y,labelsPred)))\n",
    "print('VM: ' + str(metrics.v_measure_score(y,labelsPred)))\n",
    "print('AMI: ' + str(metrics.adjusted_mutual_info_score(y,labelsPred)))\n",
    "print('ARI: ' + str(metrics.adjusted_rand_score(y,labelsPred)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle, islice \n",
    "\n",
    "fig = plt.figure()      \n",
    "colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "                                    '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "plt.scatter(x[:, 0], x[:, 1], s=10, color=colors[labelsPred.tolist()])\n",
    "plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
