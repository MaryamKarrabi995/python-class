{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "from NPIR import NPIR\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "import datetime\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType, DataType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest, desc, asc, row_number, avg, mean, least, struct, lit, sequence, sum\n",
    "from functools import reduce\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"4g\").\\\n",
    "config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\", \"4g\").appName(\"NPIR_Parallel\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read csv\n",
    "data_spark_df = spark.read.format('csv').option('header','False').option('index','False').load('blobs.csv')\n",
    "data_spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark_df = data_spark_df.select('_c0', '_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name = ['first', 'second']\n",
    "data_spark_rdd = data_spark_df.toDF(*new_name).rdd.filter(lambda x:x)\n",
    "data_spark_df = data_spark_df.toDF(*new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"False\")\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = data_spark_df.count()\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 5\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()\n",
    "chunk = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_spark = data_spark_df.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# data_spark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'old label', 'count', '1', '2')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderheadr = ['chunkLabel', 'old label', 'count']\n",
    "leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "leaderheadr = tuple(leaderheadr)\n",
    "leaderheadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'label')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsheader = ('chunkLabel', 'label')\n",
    "labelsheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "labels = labels.na.drop()\n",
    "\n",
    "leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "leaders = leaders.na.drop()\n",
    "\n",
    "ii = 0\n",
    "for z in range(0, points, chunk):\n",
    "    j = z + chunk\n",
    "    data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "    data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "    data = data.astype(float)\n",
    "    from NPIR import NPIR\n",
    "    label = NPIR(data.values,k,IR,i)\n",
    "    data['labels'] = label\n",
    "    \n",
    "    # Adding to pyspard label\n",
    "    chunklabel = np.full(len(label), ii).tolist()\n",
    "    labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "    labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "    labels = unionAll(labels, labelsDF)\n",
    "\n",
    "    leader = []\n",
    "    f = list(Cs(label))\n",
    "    f.sort()\n",
    "    for i in f:\n",
    "        leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:,:-1].T])\n",
    "    del data\n",
    "    del NPIR\n",
    "    \n",
    "    # Adding to pyspark leaders\n",
    "    for x in range(len(leader)):\n",
    "        x1 = [ii, x, Cs(label)[x]]\n",
    "        x1.extend(leader[x])\n",
    "        leader[x] = x1\n",
    "    leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "    leaders = unionAll(leaders, leaderDF)\n",
    "    ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first_numeric: float, second_numeric: float]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark = leaders.withColumn(\"first_numeric\", leaders[\"1\"].cast(FloatType()))\n",
    "data_spark = data_spark.withColumn(\"second_numeric\", data_spark[\"2\"].cast(FloatType()))\n",
    "data_spark = data_spark.drop('1').drop('2').drop('chunkLabel').drop('old label')\n",
    "count = data_spark.select('count')\n",
    "data_spark = data_spark.drop('1').drop('2').drop('chunkLabel').drop('old label').drop('count')\n",
    "data_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count: double, Cindex: int]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = count.withColumn('Cindex', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MyCheckUpdate(a, b, c, d):\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    d = float(d)\n",
    "    res = (a-c) + (b-d)\n",
    "    if res == 0:\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "check_centroid = udf(lambda x,y,z,r: MyCheckUpdate(x,y,z,r), FloatType())\n",
    "\n",
    "def squaree1(c,u,f,g):\n",
    "    c = float(c)\n",
    "    u = float(u)\n",
    "    f = float(f)\n",
    "    g = float(g)\n",
    "    array1 = np.array([c,u])\n",
    "    array2 = np.array([f,g])\n",
    "    dist = np.linalg.norm(array1-array2)\n",
    "    dist = dist.item()\n",
    "    return dist\n",
    "\n",
    "squaree_spark1 = udf(lambda x,y,z,r: squaree1(x,y,z,r), FloatType())\n",
    "sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centroid = data_spark.sample(False, 0.4,seed = 0).limit(1).cache()\n",
    "\n",
    "new_name = ['x','y']\n",
    "df_centroid = df_centroid.toDF(*new_name)\n",
    "\n",
    "#just for first round\n",
    "#first time\n",
    "i = 0\n",
    "data_cent = data_spark.join(broadcast(df_centroid))\n",
    "data_cent1 = data_cent.withColumn(str(i),squaree_spark1(data_cent.columns[0],data_cent.columns[1],\n",
    "                                              data_cent.columns[2*i+2],data_cent.columns[2*i+3]))\n",
    "data_cent2 = data_cent1.drop(data_cent1.columns[i+2]).drop(data_cent1.columns[i+3])\n",
    "data_cent3 = data_cent2.withColumn('mindist',col(str(i)))\n",
    "data_cent4 = data_cent3.withColumn('mindist1',least(data_cent3.columns[i+2], col('mindist')))\n",
    "data_cent4 = data_cent4.drop('mindist')\n",
    "data_cent5 = data_cent4.withColumnRenamed('mindist1','mindist')\n",
    "next_selected = data_cent5.orderBy(desc('mindist')).limit(1).select(data_cent5.columns[0:2])#1:3\n",
    "\n",
    "df_centroid = df_centroid.union(next_selected)\n",
    "u = [str(i)+'x',str(i)+'y']\n",
    "next_selected = next_selected.toDF(*u)\n",
    "def initial_centroids(next_selected,data_cent_5_persist, i):\n",
    "    data_cent6 = data_cent_5_persist.join(broadcast(next_selected))\n",
    "    data_cent6 = data_cent6.withColumn(str(i),squaree_spark1(data_cent6.columns[0],data_cent6.columns[1],\n",
    "                                             data_cent6.columns[i+3],data_cent6.columns[i+4]))#+4 +5\n",
    "    data_cent6 = data_cent6.drop(data_cent6.columns[i+3]).drop(data_cent6.columns[i+4])#+4 +5\n",
    "    data_cent6 = data_cent6.withColumn('mindist1',least(data_cent6.columns[i+3], col('mindist')))#4\n",
    "    data_cent6 = data_cent6.drop('mindist')\n",
    "    data_cent6 = data_cent6.withColumnRenamed('mindist1','mindist')\n",
    "    next_cent = data_cent6.orderBy(desc('mindist')).limit(1).select(data_cent6.columns[0:2])#1:3\n",
    "    return next_cent,data_cent6\n",
    "\n",
    "\n",
    "data_cent_5_persist = data_cent5.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "\n",
    "\n",
    "# start = timer()\n",
    "for i in range(1,k-1):\n",
    "    next_selected, data_cent_5_persist = initial_centroids(next_selected,data_cent_5_persist, i)\n",
    "    u = [str(i)+'x',str(i)+'y']\n",
    "    next_selected = next_selected.toDF(*u)\n",
    "\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "\n",
    "\n",
    "i= k-1\n",
    "data_cent11 = data_cent_5_persist.join(broadcast(next_selected))\n",
    "data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],data_cent11.columns[1],\\\n",
    "                                                           data_cent11.columns[k+2],data_cent11.columns[k+3]))\n",
    "data_cent11 = data_cent11.drop('mindist').drop(data_cent11.columns[k+2]).drop(data_cent11.columns[k+3])\n",
    "\n",
    "\n",
    "def FindMinCOl( *row_list):\n",
    "    ind = row_list.index(min(*row_list))\n",
    "    return int(ind)\n",
    "\n",
    "find_min_val_name = udf(FindMinCOl, IntegerType())\n",
    "data_cent14 = data_cent11.withColumn('defined_cluster', find_min_val_name(*data_cent11.columns[2:3+k]))\n",
    "data_cent14 = data_cent14.select('first_numeric','second_numeric','defined_cluster')\n",
    "\n",
    "spark.sparkContext.getConf().getAll()\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "#### Compute weighted average\n",
    "data_cent14 = data_cent14.withColumn('index', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "data_cent14 = (data_cent14.join(count, (col('Cindex') == col('index')),\\\n",
    "          \"inner\")).drop('index').drop('Cindex')\n",
    "\n",
    "for c in data_spark.columns:\n",
    "    data_cent14 = data_cent14.withColumn(c+'*',col(c)*col('count') )\n",
    "    data_cent14 = data_cent14.drop(c).withColumnRenamed(c+'*', c)\n",
    "\n",
    "new_centroid = data_cent14.groupBy('defined_cluster').sum('first_numeric', 'second_numeric').\\\n",
    "withColumnRenamed('defined_cluster', 'defined_cluster*')\n",
    "\n",
    "new_centroid1 = data_cent14.groupBy('defined_cluster').sum('count')\n",
    "new_centroid = (new_centroid1.join(new_centroid, (col('defined_cluster') == col('defined_cluster*')),\\\n",
    "          \"inner\")).drop('defined_cluster*')\n",
    "\n",
    "# for c in new_centroid.columns:\n",
    "#     new_centroid = new_centroid.withColumnRenamed(c, c.replace('sum(', '').replace(')', ''))\n",
    "\n",
    "for c in new_centroid.columns[2:]:\n",
    "    new_centroid = new_centroid.withColumn(c, col(c) / col('sum(count)'))\n",
    "\n",
    "new_centroid = new_centroid.drop('sum(count)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "def UpdateCentroid(x):\n",
    "    data_cent_join1 = data_spark.join(broadcast(x))\n",
    "    data_cent_join2 = data_cent_join1.withColumn('dist',squaree_spark1(data_cent_join1.columns[0],\n",
    "                                                                 data_cent_join1.columns[1],\n",
    "                                       data_cent_join1.columns[3],data_cent_join1.columns[4]))#3 4\n",
    "    w = Window.partitionBy(data_cent_join2.columns[1])\n",
    "    next_centroid = data_cent_join2.withColumn('mindist', F.min('dist').over(w)).\\\n",
    "    filter(col('dist') == col('mindist')).drop('dist')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Compute weighted average\n",
    "    data_cent14 = next_centroid.withColumn('index', row_number().\\\n",
    "                                              over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "    data_cent14 = (data_cent14.join(count, (col('Cindex') == col('index')),\\\n",
    "              \"inner\")).drop('index').drop('Cindex')\n",
    "\n",
    "    for c in data_spark.columns:\n",
    "        data_cent14 = data_cent14.withColumn(c+'*',col(c)*col('count') )\n",
    "        data_cent14 = data_cent14.drop(c).withColumnRenamed(c+'*', c)\n",
    "\n",
    "    new_centroid = data_cent14.groupBy('defined_cluster').sum('first_numeric', 'second_numeric').\\\n",
    "    withColumnRenamed('defined_cluster', 'defined_cluster*')\n",
    "\n",
    "    new_centroid1 = data_cent14.groupBy('defined_cluster').sum('count')\n",
    "    new_centroid = (new_centroid1.join(new_centroid, (col('defined_cluster') == col('defined_cluster*')),\\\n",
    "              \"inner\")).drop('defined_cluster*')\n",
    "\n",
    "\n",
    "    for c in new_centroid.columns[2:]:\n",
    "        new_centroid = new_centroid.withColumn(c, col(c) / col('sum(count)'))\n",
    "\n",
    "    update_new_centroid = new_centroid.drop('sum(count)')\n",
    "    ############\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return update_new_centroid, next_centroid\n",
    "\n",
    "new_centroid_persist = new_centroid.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# start = timer()\n",
    "\n",
    "for i in range(20):\n",
    "    new_centroid_persist, final_data = UpdateCentroid(new_centroid_persist)\n",
    "\n",
    "# end = timer()\n",
    "\n",
    "\n",
    "final_data = final_data.withColumnRenamed('avg(first_numeric)','cent_x').\\\n",
    "withColumnRenamed('avg(second_numeric)','cent_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centroid_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cent11 = sc.parallelize([])\n",
    "data_cent11 = 0\n",
    "for i in range(k):\n",
    "    u = [ str(i)+'x',str(i)+'y']\n",
    "    next_selected = new_centroid_persist.filter(col('defined_cluster') == str(i)).\\\n",
    "    drop('defined_cluster').toDF(*u)\n",
    "    if i == 0:\n",
    "        data_cent11 = data_spark_df.join(broadcast(next_selected))\n",
    "#         print(data_cent.count())\n",
    "        \n",
    "        data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],\\\n",
    "                            data_cent11.columns[1],data_cent11.columns[i+2],data_cent11.columns[i+3]))\n",
    "        data_cent11 = data_cent11.drop(data_cent11.columns[i+2]).drop(data_cent11.columns[i+3])\n",
    "        data_cent11 = data_cent11.withColumn('mindist',col(str(i)))\n",
    "        data_cent11 = data_cent11.withColumn('mindist1',least(data_cent11.columns[i+2], col('mindist')))\n",
    "        data_cent11 = data_cent11.drop('mindist')\n",
    "        data_cent11 = data_cent11.withColumnRenamed('mindist1','mindist')\n",
    "    elif i > 0:\n",
    "        data_cent11 = data_cent11.join(broadcast(next_selected))\n",
    "        data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],\\\n",
    "                                    data_cent11.columns[1], data_cent11.columns[i+2],data_cent11.columns[i+3]))\n",
    "        data_cent11 = data_cent11.drop(u[0]).drop(u[1])\n",
    "        data_cent11 = data_cent11.withColumn('mindist1',least(data_cent11.columns[i+3], col('mindist')))#4\n",
    "        data_cent11 = data_cent11.drop('mindist')\n",
    "        data_cent11 = data_cent11.withColumnRenamed('mindist1','mindist')\n",
    "#         next_cent = data_cent11.orderBy(desc('mindist')).limit(1).select(data_cent11.\\\n",
    "#                                                                          columns[:len(data_spark_df.columns)-1])#1:3\n",
    "data_cent11 = data_cent11.drop('mindist')\n",
    "\n",
    "def FindMinCOl( *row_list):\n",
    "    ind = row_list.index(min(*row_list))\n",
    "    return int(ind)\n",
    "find_min_val_name = udf(FindMinCOl, IntegerType())\n",
    "\n",
    "data_cent11 = data_cent11.withColumn('defined_cluster', find_min_val_name(*data_cent11.columns[2:3+k]))\n",
    "data_cent11 = data_cent11.select('first','second','defined_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_cent11.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data = final_data.withColumn('index', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1).\\\n",
    "drop('mindist').drop('second_numeric').drop('first_numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaders = leaders.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaders1 = leaders.drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_final = (leaders1\n",
    "    .join(final_data, (col('index_column_name') == col('index')), \"inner\")).drop('1').drop('2').\\\n",
    "    drop('index_column_name').drop('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_final = lead_final.withColumnRenamed('chunkLabel','chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lead_final = (labels\n",
    "    .join(lead_final,  ((col('label') == col('old label')) & (col('chunkLabel') == col('chunk'))),\\\n",
    "          \"left\")).drop('chunkLabel').drop('label').drop('old label').drop('chunk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = label_lead_final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
