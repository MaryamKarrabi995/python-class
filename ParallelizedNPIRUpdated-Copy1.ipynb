{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### multiprocessing\n",
    "from multiprocessing.pool import Pool\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "########\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType, DataType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest, desc, asc, row_number, avg, mean, least, struct, lit, sequence, sum\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from scipy.spatial import distance\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"14g\").\\\n",
    "config(\"spark.default.parallelism\", \"200\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\", \"14g\").appName(\"NPIR_Parallel\").\\\n",
    "config(\"spark.executor.memory\", \"14g\").config(\"spark.driver.memory\", \"14g\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "x1,y1 = make_blobs(n_samples=5000, centers=3, n_features=2,\n",
    "                random_state=1)\n",
    "data = pd.DataFrame(x1)\n",
    "data['label'] = y1\n",
    "data.head()\n",
    "\n",
    "data = shuffle(data)\n",
    "data.head()\n",
    "\n",
    "data.to_csv('blobs3.csv', index=False)\n",
    "\n",
    "#read csv\n",
    "data_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('blobs3.csv')\n",
    "# data_spark_df.show()\n",
    "# data_spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark_df = data_spark_df.select(data_spark_df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"False\")\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = data_spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()\n",
    "chunk = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_spark = data_spark_df.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# data_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'old label', '1', '2')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderheadr = ['chunkLabel', 'old label']\n",
    "leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "leaderheadr = tuple(leaderheadr)\n",
    "leaderheadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'label')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsheader = ('chunkLabel', 'label')\n",
    "labelsheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:02:14.999358\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "\n",
    "# labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "# labels = labels.na.drop()\n",
    "\n",
    "leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "leaders = leaders.na.drop()\n",
    "\n",
    "ii = 0\n",
    "for z in range(0, points, chunk):\n",
    "    j = z + chunk\n",
    "    data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "    data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "    data = data.astype(float)\n",
    "    from NPIR import NPIR\n",
    "    label = NPIR(data.values,k,IR,i)\n",
    "    data['labels'] = label\n",
    "    \n",
    "    # Adding to pyspard label\n",
    "#     chunklabel = np.full(len(label), ii).tolist()\n",
    "#     labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "#     labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "#     labels = unionAll(labels, labelsDF)\n",
    "\n",
    "    leader = []\n",
    "    f = list(Cs(label))\n",
    "    f.sort()\n",
    "    for i in f:\n",
    "        leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:,:-1].T])\n",
    "    del data\n",
    "    del NPIR\n",
    "    \n",
    "    # Adding to pyspark leaders\n",
    "    for x in range(len(leader)):\n",
    "        x1 = [ii, x]\n",
    "        x1.extend(leader[x])\n",
    "        leader[x] = x1\n",
    "    leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "    leaders = unionAll(leaders, leaderDF)\n",
    "    ii += 1\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Euclidean_distance(a,b,c,d):\n",
    "#     a = float(a)\n",
    "#     b = float(b)\n",
    "#     c = float(c)\n",
    "#     d = float(d)\n",
    "#     array1 = np.array([a,b])\n",
    "#     array2 = np.array([c,d])\n",
    "#     dist = np.linalg.norm(array1-array2)\n",
    "#     dist = dist.item()\n",
    "#     return dist\n",
    "\n",
    "# Euclidean_distance_spark = udf(lambda x,y,z,t: Euclidean_distance(x,y,z,t), FloatType())\n",
    "# sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# leaders.toPandas().to_csv('leaders.csv')\n",
    "# leaders_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('leaders.csv')\n",
    "# parsedData = leaders_spark_df.select(leaders.columns[2:]).rdd.map(list)\n",
    "# clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# leaders.write.format('csv').option('header',True).save('leaders2.csv')\n",
    "# leaders_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('leaders2.csv')\n",
    "# parsedData = leaders_spark_df.select(leaders.columns[2:]).rdd.map(list)\n",
    "# clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:01:42.999464\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "leaders.select(leaders.columns[2:]).toPandas().to_csv('leaders.csv')\n",
    "leaders_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('leaders.csv')\n",
    "parsedData = leaders_spark_df.select(leaders.columns[2:]).rdd.map(list)\n",
    "clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# # parsedData = leaders.select(['1', '2']).rdd.map(list)\n",
    "# leaders.select(leaders.columns[2:]).toPandas().to_csv('leaders.csv')\n",
    "# leaders_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('leaders.csv')\n",
    "# # leaders_spark_df = leaders_spark_df.select('1', '2')\n",
    "# parsedData = leaders_spark_df.rdd.map(list)\n",
    "# # Build the model (cluster the data)\n",
    "# clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del leaders_spark_df\n",
    "del parsedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# parsedData = leaders.select(leaders.columns[2:]).rdd.map(list)\n",
    "# clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "spark_cluster_centroid = sqlContext.createDataFrame(([c.tolist() for c in clusters.centers]),)\n",
    "spark_cluster_centroid = spark_cluster_centroid.withColumn('defined_cluster', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCol = data_spark_df.columns\n",
    "for i in range(k):\n",
    "    if i == 0:\n",
    "        centerOfiTH = clusters.centers[i].tolist()#for example, the entry against which you want distances\n",
    "        distance_udf = F.udf(lambda x: float(distance.euclidean([float(z) for z in x], centerOfiTH)),\\\n",
    "                             FloatType())\n",
    "        columns = [F.col(c) for c in dataCol]\n",
    "        data_cent = data_spark_df.withColumn('dis' + str(i) + 'th', distance_udf(F.array(columns)))\n",
    "        data_cent = data_cent.withColumn('mindist', col('dis' + str(i) + 'th'))\n",
    "        data_cent\n",
    "        data_cent = data_cent.withColumn('mindist1', least(col('dis' + str(i) + 'th'), col('mindist')))\n",
    "        data_cent = data_cent.drop('mindist')\n",
    "    #     .drop('dis' + str(i) + 'th')\n",
    "        data_cent = data_cent.withColumnRenamed('mindist1', 'mindist')\n",
    "    elif i > 0:\n",
    "        centerOfiTH = clusters.centers[i].tolist()#for example, the entry against which you want distances\n",
    "        distance_udf = F.udf(lambda x: float(distance.euclidean([float(z) for z in x], centerOfiTH)),\\\n",
    "                             FloatType())\n",
    "        columns = [F.col(c) for c in dataCol]\n",
    "        data_cent = data_cent.withColumn('dis' + str(i) + 'th', distance_udf(F.array(columns)))\n",
    "        data_cent = data_cent.withColumn('mindist1', least(col('dis' + str(i) + 'th'), col('mindist')))#4\n",
    "        data_cent = data_cent.drop('mindist')\n",
    "    #     .drop('dis' + str(i) + 'th')\n",
    "        data_cent = data_cent.withColumnRenamed('mindist1', 'mindist')\n",
    "data_cent = data_cent.drop('mindist')\n",
    "\n",
    "# data_cent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Min_COl( *row_list):\n",
    "    ind = row_list.index(min(*row_list))\n",
    "    return int(ind)\n",
    "find_min_val_name = udf(Min_COl, IntegerType())\n",
    "\n",
    "data_cent = data_cent.withColumn('defined_cluster', find_min_val_name(*data_cent.\\\n",
    "            columns[len(data_spark_df.columns):len(data_spark_df.columns) + 1 + k]))\n",
    "# data_cent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:01.345929\n"
     ]
    }
   ],
   "source": [
    "d = data_cent.select('defined_cluster').toPandas()\n",
    "\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('blobs3.csv')\n",
    "# data.head()\n",
    "data = data[['0','1','label']]\n",
    "data['new'] = d['defined_cluster'].astype('int8')\n",
    "# data.head()\n",
    "y = data['label']\n",
    "labelsPred = data['new']\n",
    "x = data[['0','1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measures:\n",
      "HS: 0.9698386337531651\n",
      "CS: 0.9699158693282405\n",
      "VM: 0.969877250003051\n",
      "AMI: 0.9698735929087531\n",
      "ARI: 0.9811788137994254\n"
     ]
    }
   ],
   "source": [
    "print('Measures:')\n",
    "print('HS: ' + str(metrics.homogeneity_score(y,labelsPred)))\n",
    "print('CS: ' + str(metrics.completeness_score(y,labelsPred)))\n",
    "print('VM: ' + str(metrics.v_measure_score(y,labelsPred)))\n",
    "print('AMI: ' + str(metrics.adjusted_mutual_info_score(y,labelsPred)))\n",
    "print('ARI: ' + str(metrics.adjusted_rand_score(y,labelsPred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
