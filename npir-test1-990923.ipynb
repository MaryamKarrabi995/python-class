{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count point:  10\n",
      "points:  [(2, 4), (4, 5), (3, 8), (6, 7), (1, 5), (6, 2), (3, 7), (8, 9), (5, 8), (3, 6)]\n",
      "+---------+\n",
      "|idx_point|\n",
      "+---------+\n",
      "|        0|\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "+---------+\n",
      "\n",
      "+---------+-------------+\n",
      "|idx_point|ind_nxt_point|\n",
      "+---------+-------------+\n",
      "|        0|            0|\n",
      "|        1|            0|\n",
      "|        2|            0|\n",
      "|        3|            0|\n",
      "|        4|            0|\n",
      "|        5|            0|\n",
      "|        6|            0|\n",
      "|        7|            0|\n",
      "|        8|            0|\n",
      "|        9|            0|\n",
      "+---------+-------------+\n",
      "\n",
      "initial_Point:  [3, 8, 0]\n",
      "ls_clu_Pnt:  [[0, -1], [1], [2], [3, -1], [4], [5], [6], [7], [8, -1], [9]]\n",
      "ls_sel_pnt:  [3, 8, 0]\n",
      "*****************start clustering********************\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "# Big Data Programming \n",
    "# Lab 2 - Spark \n",
    "\n",
    "\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "from math import sqrt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "#from pyspark.sql.functions import lit\n",
    "#from pyspark.sql.functions import min\n",
    "\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.functions import udf, col , min\n",
    "\n",
    "from scipy import spatial\n",
    "from treelib import Tree\n",
    "\n",
    "\n",
    "# ---------- helper functions ----------    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# ----------------- Main Program ---------------\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"NBA-kmeans\").getOrCreate()\n",
    "#sc=spark.sparkContext\n",
    "\n",
    "#--error timeout\n",
    "import pyspark as spark\n",
    "#conf = spark.SparkConf().setAppName(\"applicaiton\").set(\"spark.executor.heartbeatInterval\", \"200000\").set(\"spark.network.timeout\", \"300000\")\n",
    "#sc = spark.SparkContext.getOrCreate(conf)\n",
    "#sqlcontext = spark.SQLContext(sc)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SQL_DataFrame\")\\\n",
    "  .master(\"local\")\\\n",
    "  .config(\"spark.network.timeout\", \"90000\")\\\n",
    "  .config(\"spark.executor.heartbeatInterval\", \"80000\")\\\n",
    "  .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "#import pyspark as spark\n",
    "#conf = spark.SparkConf().setMaster(\"yarn-client\").setAppName(\"sparK-mer\")\n",
    "#conf.set(\"spark.executor.heartbeatInterval\",\"3600s\")\n",
    "#conf.set(\"spark.executor.heartbeatInterval\",\"120s\")\n",
    "#sc = spark.SparkContext('local[2]', '', conf=conf) # uses 4 cores on your local machine\n",
    "#---\n",
    "# ---- load and preprocessing -------\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"./shot_logs.csv\", header = \"true\", inferSchema = \"true\")\n",
    "##dataPts = df.filter(df.player_name == 'james harden').select('SHOT_DIST','CLOSE_DEF_DIST').na.drop()\n",
    "##dataRDD = dataPts.rdd.map(lambda r: (r[0], r[1], r[2]))\n",
    "\n",
    "#*************** count point \n",
    "#dataPts = df.filter(df.player_name == 'james harden').select('SHOT_DIST','CLOSE_DEF_DIST').na.drop()#count points:1054 point\n",
    "#dataPts = df.select('SHOT_DIST','CLOSE_DEF_DIST').na.drop() #count points: 128069 point\n",
    "#coords = dataPts.rdd.map(lambda r: (r[0], r[1])).collect()  \n",
    "coords = [(2, 4),(4, 5), (3,8), (6,7), (1,5), (6,2), (3,7), (8,9), (5,8), (3,6)]\n",
    "#***************\n",
    "\n",
    "#dataRDD = dataPts.rdd.map(lambda r: (r[0], r[1]))\n",
    "##print(dataRDD)\n",
    "#print(dataRDD.take(dataRDD.count()))\n",
    "#print(dataRDD.take(5))\n",
    "\n",
    "\n",
    "dataRDD = sc.parallelize(coords)\n",
    "#count_point=len(coords)\n",
    "\n",
    "#rdd = rdd.map(lambda x: [float(i) for i in x])\n",
    "#print(dataRDD.take(1))\n",
    "count_point=len(dataRDD.take(dataRDD.count()))\n",
    "print(\"count point: \",len(dataRDD.take(dataRDD.count())))\n",
    "print(\"points: \",dataRDD.take(dataRDD.count())) #all \n",
    "# -------- initialization ---------\n",
    "\n",
    "\n",
    "rdd_length = dataRDD.map(lambda x: len(x))\n",
    "rdd_length.collect()\n",
    "#print(rdd_length.take(rdd_length.count()))\n",
    "len_vec=rdd_length.take(1)[0] #len demention(features)\n",
    "#count_point=len(coords)\n",
    "#print(len_vec)\n",
    "\n",
    "\n",
    "totalindex=0\n",
    "ir=0.2\n",
    "index=0\n",
    "iters = 0 \n",
    "#old_centroid = int_centroid\n",
    "#old_centroid = coords\n",
    "k=3\n",
    "run=1\n",
    "\n",
    "#list_index_nearset=list(range(0, count_point) )\n",
    "list_index_nearset=[0]*count_point\n",
    "#print(\"list_index_nearset\",list_index_nearset)\n",
    "\n",
    "for m in range(run):\n",
    "\n",
    "    \n",
    "    schema = StructType([StructField('id', IntegerType(), True)])\n",
    "    df = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\n",
    "    df = spark.range(0, count_point).withColumnRenamed(\"id\", \"idx_point\")\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    df= df.withColumnRenamed('_1','idx_point')\n",
    "\n",
    "    #df8 = df.rdd.map(lambda x: [r for r in row(x)]).collect() \n",
    "    #print(df8)\n",
    "    \n",
    "    #from pyspark.ml.feature import VectorAssembler\n",
    "    #FEATURES_COL = df.columns[1:]\n",
    "    ##print(FEATURES_COL)\n",
    "    #vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "    #df = vecAssembler.transform(df).select('idx_point', \"features\")\n",
    "    df= df.withColumn('ind_nxt_point', F.lit(0))\n",
    "\n",
    "\n",
    "    df.show()\n",
    "\n",
    "    #def make_list(col):\n",
    "        #return list(map(int,[x for x in range(col+1) if x % 2 == 0]))    \n",
    "    #make_list_udf = udf(make_list, ArrayType(IntegerType()))\n",
    "    #df2 = df.withColumn('features',make_list_udf(col('features')))    \n",
    "    \n",
    "    ls_clu_Pnt=[] #list E:N,N,N..\n",
    "    ls_sel_pnt=[] #initial points & clustred points\n",
    "    ls_clu_Pnt = df.select('idx_point').rdd.flatMap(lambda x: [x]).collect()  #list E:N,N,N..\n",
    "    ls_clu_Pnt= [ele.__getattr__('idx_point') for ele in ls_clu_Pnt] #convert row to one list\n",
    "    ls_clu_Pnt= [ls_clu_Pnt[i:i+1] for i in range(0, len(ls_clu_Pnt), 1)] #convert list 1d to list 2d\n",
    "    #print('ls_clu_Pnt: ',ls_clu_Pnt.__str__()) # all element print\n",
    "\n",
    "    initial_Point = list(map(lambda row: row[0], df.select('idx_point').rdd.takeSample(False, 3))) #convert row out to one number\n",
    "    print('initial_Point: ',initial_Point)\n",
    "    #points_in_cluster = initial_Point\n",
    "    #ls_sel_pnt= initial_Point #list: all selected points for clustering(selected point in all clutser)\n",
    "    ls_sel_pnt.append(initial_Point[0])\n",
    "    ls_sel_pnt.append(initial_Point[1])\n",
    "    ls_sel_pnt.append(initial_Point[2])\n",
    "    \n",
    "    ls_clu_Pnt[ls_sel_pnt[0]].append(-1)\n",
    "    ls_clu_Pnt[ls_sel_pnt[1]].append(-1)\n",
    "    ls_clu_Pnt[ls_sel_pnt[2]].append(-1)\n",
    "    print('ls_clu_Pnt: ',ls_clu_Pnt.__str__()) # all element print\n",
    "    print('ls_sel_pnt: ',ls_sel_pnt)\n",
    "    #df = spark.createDataFrame(points_in_cluster, IntegerType()) #convert list to df\n",
    "    #df2 = spark.createDataFrame([(\"Range 1\", list([101,105])), (\"Range 2\", list([200, 203]))])\n",
    "\n",
    "    #----------------part 2-----------\n",
    "    print(\"*****************start clustering********************\")\n",
    "          \n",
    "    \n",
    "\n",
    "    \n",
    "    tree = spatial.cKDTree(coords,leafsize=16)\n",
    "    kdt_b = sc.broadcast(tree)\n",
    "    #distance,index = kdt_b.value.query([2.4, 4], k=2)\n",
    "    #print(distance,index)    \n",
    "    #print(index[1])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------index:  1\n",
      "-------------------------------------------index:  2\n",
      "-------------------------------------------index:  3\n",
      "-------------------------------------------index:  4\n",
      "-------------------------------------------index:  5\n",
      "-------------------------------------------index:  6\n",
      "-------------------------------------------index:  7\n",
      "-------------------------------------------index:  8\n",
      "-------------------------------------------index:  9\n",
      "-------------------------------------------index:  10\n",
      "list_all_full_cluster: [[0, 4, 1, 6, 9], [3, 8, 7, 2], [5]]\n"
     ]
    }
   ],
   "source": [
    "    #---990923\n",
    "    def deep_index(lst, w , frs_sec_idx, end_idx): #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "        return [(i, sub.index(w)) for (i, sub) in enumerate(lst) if w in sub[frs_sec_idx:end_idx]] #[1:] =>num elemnt of witch cluster?\n",
    "    #def deep_index2(lst, w): #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "    #    return [(i, sub.index(w)) for (i, sub) in enumerate(lst) if w in sub[0:1]] \n",
    "\n",
    "\n",
    "    def clu_update(vec,num1):\n",
    "        tmp_point= vec[0]\n",
    "        if tmp_point==elec_point and (num1 not in vec):\n",
    "            vec.append(num1)       \n",
    "        #if (num1 in ls_sel_pnt):\n",
    "            #l=[[0], [1], [2], [3, 8], [4.0], [5], [6], [7, 3, 8], [8, 4], [9]]\n",
    "            ##print('l: ',l.__str__())\n",
    "            #Asign_point= deep_index(l, 3)\n",
    "            #Asign_point= deep_index(ls_clu_Pnt, num1) #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "            ##print(Asign_point) =>[(7, 1)] (1->index 3)\n",
    "            #Asign_point=Asign_point[0][0]\n",
    "            ##print(Asign_point) => 7 (7 -> cluster dad(Asign))\n",
    "\n",
    "            #user_func = udf (lambda x,y: [i for i, e in enumerate(x) if e==y ]) #find num index in list in column features\n",
    "            #df_idx_dad_min_dis = df.filter((df['idx_point'] == Asign_point) | (df['idx_point'] == elec_point))\n",
    "            #df_idx_dad_min_dis = df.filter(df['idx_point'] == 7) \n",
    "            #df5=df\n",
    "            #    .select(df['idx_point'], df['features'])\\\n",
    "            #    .withColumn('item_position',user_func(df.features,F.lit(num1)))\\\n",
    "            #    .orderBy(\"item_position\", ascending=True).limit(1).collect()\n",
    "            #idx_dad_min_dis = df_idx_dad_min_dis[0][0]\n",
    "            ##print(idx_dad_min_dis[0][0])       \n",
    "            #vec[idx_dad_min_dis:-1].append(num1) \n",
    "        return vec       \n",
    "\n",
    "    def indexnext(idx_point1, ind_nxt_point1):\n",
    "        d=ind_nxt_point1\n",
    "        if idx_point1 == elec_point: \n",
    "            if (d<count_point):\n",
    "                d= d+1\n",
    "            else: d=0    \n",
    "        return d\n",
    "    \n",
    "    def update_father_child(x,tmp_next_Points,father_point):\n",
    "        if ((len(x)>2) and (tmp_next_Points in x[1:2])):\n",
    "            x[1]=father_point\n",
    "        return x\n",
    "\n",
    "                        \n",
    "    def child_other_father(x1,list_num_childs1):\n",
    "        if (len(x1)>2):\n",
    "            if x1[0] not in list_num_childs1:  \n",
    "                return (x1[0]) \n",
    "        return -1\n",
    "    \n",
    "\n",
    "    #print('ls_clu_Pnt: ',ls_clu_Pnt.__str__()) # all element #print\n",
    "    #print('ls_sel_pnt: ',ls_sel_pnt)\n",
    "\n",
    "    #totoalindex=round(ir*(count_point * count_point),0)\n",
    "    ir=0.2\n",
    "    totoalindex=round(ir*(count_point * count_point),0)\n",
    "    #print(\"totoalindex:\", totoalindex)\n",
    "    \n",
    "    index=0\n",
    "    i=1\n",
    "    while (len(ls_sel_pnt)< count_point) and (index<totoalindex):\n",
    "    #while i<=100:\n",
    "    #while i==1:\n",
    "        rdd_sel_pnt=sc.parallelize(ls_sel_pnt)\n",
    "        #elec_point =  sc.parallelize(ls_sel_pnt).takeSample(False, 1)\n",
    "        elec_point =  rdd_sel_pnt.takeSample(False, 1)\n",
    "        elec_point= elec_point[0]\n",
    "        rdd_cluster_pnt = sc.parallelize(ls_clu_Pnt)\n",
    "        #elec_point=0\n",
    "\n",
    "        \n",
    "        \n",
    "        index=index+1\n",
    "        print(\"-------------------------------------------index: \",index)\n",
    "        #print('elec_point: ',elec_point)\n",
    "        \n",
    "        \n",
    "        #filter_udf = udf(indexnext, IntegerType())\n",
    "        #df= df.withColumn(\"ind_nxt_point\", filter_udf(df['idx_point'], df['ind_nxt_point'])) #update index for point next\n",
    "        #df.show()\n",
    "        #df = df.filter(col(\"idx_point\").between(0, 0)).withColumn('ind_nxt_point',F.when(F.col('idx_point') == elec_point, 55).otherwise(88))\n",
    "        #tmp_index_nearset = df.filter(df['idx_point'] == elec_point).collect()[0][1] #+1:first point is self\n",
    "        #df.show()\n",
    "        \n",
    "        list_index_nearset[elec_point]=list_index_nearset[elec_point]+1\n",
    "        ##print(\"list_index_nearset\",list_index_nearset)\n",
    "        tmp_index_nearset=list_index_nearset[elec_point]\n",
    "        \n",
    "        \n",
    "        ##print(\"tmp_index_nearset: \",tmp_index_nearset)\n",
    "        \n",
    "        \n",
    "\n",
    "        distance,index_dis = kdt_b.value.query(coords[elec_point], k=tmp_index_nearset+1)#+1:first point is self\n",
    "        #distance,index_dis = kdt_b.value.query(coords[elec_point], k=7)\n",
    "        ##print(distance,index_dis)  \n",
    "        ##print(\"vec_dis:\",index_dis)  \n",
    "        #tmp_next_Points=coords[index_dis[tmp_index_nearset]]\n",
    "        tmp_next_Points=index_dis[tmp_index_nearset]\n",
    "        ##print(index_dis[tmp_index_nearset])        \n",
    "        #print('tmp_next_Points: ',tmp_next_Points)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        #rdd= sc.parallelize(ls_clu_Pnt)\n",
    "        #ls_clu_Pnt = rdd.map(lambda x: clu_update(x,tmp_next_Points)).collect()\n",
    "        if  tmp_next_Points in ls_sel_pnt: #N befor selected\n",
    "            #----\n",
    "            #if (elec_point in ls_sel_pnt) and (len(ls_sel_pnt)==3): continue #run 1 and two point are in initial_point\n",
    "            #Asign_point= deep_index(ls_clu_Pnt, elec_point, 0,1)[0][0] # jsut col 0 returned for check\n",
    "            if  elec_point not in ls_clu_Pnt[tmp_next_Points][2:]: #check (4,0) for (0,4) ,check else col 2 is father\n",
    "\n",
    "                \n",
    "                #rdd = sc.parallelize(ls_clu_Pnt)#find elect father (check for e & n are in samecluster(no assignment))\n",
    "                rdd = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: (len(x)>2) and (elec_point in x[2:])).collect()\n",
    "                if (len(rdd) >0): \n",
    "                    if rdd[0][1] == -1: father_elect= rdd[0][0]\n",
    "                    else:  father_elect= rdd[0][1]\n",
    "                else:  father_elect=elec_point\n",
    "                #rdd = sc.parallelize(ls_clu_Pnt)#find tmp_next_Points father (check for e & n are in samecluster(no assignment))\n",
    "                rdd = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: (len(x)>2) and (tmp_next_Points in x[2:])).collect()\n",
    "                if (len(rdd) >0): \n",
    "                    if rdd[0][1] == -1: father_tmp_next_Points= rdd[0][0]\n",
    "                    else:  father_tmp_next_Points= rdd[0][1]\n",
    "                else:  father_tmp_next_Points=tmp_next_Points\n",
    "                    \n",
    "                \n",
    "                #rdd = sc.parallelize(ls_clu_Pnt)#check for e & n are in samecluster(no assignment) \n",
    "                #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:]) and (tmp_next_Points2 in x[2:])) ).collect()\n",
    "                #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2).filter(lambda x: elec_point in x[2:] and (tmp_next_Points2 in x[2:])) ).collect()\n",
    "                #if (len(rdd) ==0):\n",
    "                if (father_elect != father_tmp_next_Points):\n",
    "                \n",
    "                    Asign_point= deep_index(ls_clu_Pnt, tmp_next_Points, 2,len(ls_clu_Pnt)) #find num index in cluster(ls_clu_Pnt[2:] ,for two father)\n",
    "                    #print('Asign_point(befor father): ',Asign_point) #=>[(7, 1)] (7 -> col 0(father) , 1 -> index tmp_next_Points in [])\n",
    "                \n",
    "                    if len(Asign_point)>0: #N befor selected and is not one of initial point\n",
    "                        #print(\"N befor selected and is not one of initial point\")\n",
    "                        father_befor=Asign_point[0][0]\n",
    "                        father_new= elec_point  \n",
    "                        #point_father = [coords[father_befor],coords[father_new]]\n",
    "                        tree_father = spatial.cKDTree([coords[father_befor],coords[father_new]],leafsize=16)\n",
    "                        distance,index_dis = tree_father.query(coords[tmp_next_Points])\n",
    "                        ##print(distance,index_dis)\n",
    "                        if (index_dis==0): \n",
    "                            idx_dad_min_dis=father_befor \n",
    "                            idx_dad_max_dis=father_new \n",
    "                        else: \n",
    "                            idx_dad_min_dis=father_new \n",
    "                            idx_dad_max_dis=father_befor \n",
    "                        #print(\"idx_dad_min_dis: \",idx_dad_min_dis)\n",
    "                        \n",
    "                        if (tmp_next_Points) not in ls_clu_Pnt[idx_dad_min_dis][2:]:\n",
    "                            #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                            rdd = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                            if (len(rdd) >0):\n",
    "                                if rdd[0][1] == -1: \n",
    "                                    father_point= rdd[0][0]\n",
    "                                else:\n",
    "                                    father_point= rdd[0][1]\n",
    "                                ls_clu_Pnt[elec_point].append(father_point)  #(col1:point number , col2: father point, col3: N ...)     \n",
    "                            else:\n",
    "                                if (len(ls_clu_Pnt[idx_dad_min_dis])==1):                \n",
    "                                    ls_clu_Pnt[idx_dad_min_dis].insert(1,-1)\n",
    "                            #print(\"end 5\")\n",
    "                            ls_clu_Pnt[idx_dad_min_dis].append(tmp_next_Points) \n",
    "                            if (tmp_next_Points in ls_clu_Pnt[idx_dad_max_dis][2:]):\n",
    "                                ls_clu_Pnt[idx_dad_max_dis].remove(tmp_next_Points) \n",
    "                                if ((len(ls_clu_Pnt[idx_dad_max_dis])==2) and (ls_clu_Pnt[idx_dad_max_dis][1] !=-1)):\n",
    "                                    ls_clu_Pnt[idx_dad_max_dis].remove(ls_clu_Pnt[idx_dad_max_dis][1]) #remove child => len=2 => remve childa's father\n",
    "                            #**************---update childs(find childs & assign other father(elected)    \n",
    "                            #--find elected's father \n",
    "                            #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                            rdd = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                            if (len(rdd) >0):\n",
    "                                if rdd[0][1] == -1: \n",
    "                                    father_point= rdd[0][0]\n",
    "                                else:\n",
    "                                    father_point= rdd[0][1]\n",
    "                                #if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                                    #ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                                #ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                            else:\n",
    "                                #if (elec_point not in initial_Point):\n",
    "                                #ls_clu_Pnt[elec_point].insert(1,-1)\n",
    "                                father_point=-1\n",
    "                            #--\n",
    "                            if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                                #--find list childs of childs tmp_next_Points & update fathers\n",
    "                                #print(\"father point:\",father_point)\n",
    "                                ls_clu_Pnt = rdd_cluster_pnt.map(lambda x: update_father_child(x,tmp_next_Points,father_point)).collect()#[1:2] searech all items with index[1] only\n",
    "                                \n",
    "                                #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                                #list_childs = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: ((len(x)>2) and (tmp_next_Points in x[1:2])) ).collect()#[1:2] searech all items with index[1] only\n",
    "                                ##print('list_childs of childs ',tmp_next_Points,': ', list_childs)\n",
    "                                #if (len(list_childs) >0):\n",
    "                                #    for x in list_childs: x[1]= father_point\n",
    "                                #--\n",
    "                                #--remove (befor electe) and reasign new elected\n",
    "                                if (ls_clu_Pnt[tmp_next_Points][1]==-1):\n",
    "                                    initial_Point.remove(tmp_next_Points)\n",
    "                                    elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                                    elec_point_again= elec_point_again[0]\n",
    "                                    #print('elec_point_again: ',elec_point_again)\n",
    "                                    ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                                    ls_sel_pnt.append(elec_point_again)\n",
    "                                    initial_Point.append(elec_point_again)  \n",
    "                                    #print('initial_Point new:', initial_Point)                            \n",
    "                                #--\n",
    "                            #if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                            #    ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                            #ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                            #if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                            #    ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                            #***************------   \n",
    "                            \n",
    "                            #----update childs(find childs & assign to other elected)    \n",
    "                            ##def child_other_father(x1,list_num_childs1):\n",
    "                            ##    if (len(x1)>2):\n",
    "                            ##        if x1[0] not in list_num_childs1:  \n",
    "                            ##            return (x1[0]) \n",
    "                            ##    return -1\n",
    "                            #if len(ls_clu_Pnt[tmp_next_Points])>2:\n",
    "                            #    idx_cluster_child= ls_clu_Pnt[tmp_next_Points][1]\n",
    "                            #    #print('idx_cluster_child: ',idx_cluster_child)\n",
    "                            #    rdd2 = sc.parallelize(ls_clu_Pnt)#select all points of cluster father(col2)-> find child_other_father and remove them\n",
    "                            #    list_samecluster_full_childs = rdd2.map(lambda x: x).filter(lambda x: ((len(x)>2) and (idx_cluster_child in x[1:2]) )).collect() #find index father (in col 2 of list)\n",
    "                            #    #print('list_samecluster_full_childs:' , list_samecluster_full_childs)\n",
    "                            #    list_num_childs=[]\n",
    "                            #    for x in list_samecluster_full_childs: list_num_childs.append(x[2:]) #all child's\n",
    "                            #    l= sum(list_num_childs, []) #convert [[4,5],[3]] to [4,5,3]\n",
    "                            #    #print('list_num_childs: ',list_num_childs)\n",
    "                            #    rdd = sc.parallelize(list_samecluster_full_childs)\n",
    "                            #    list_child_other_father = rdd.map(lambda x: child_other_father(x, list_num_childs)).collect()\n",
    "                            #    #print('list_child_other_father: ',list_child_other_father)\n",
    "                            #    while -1 in list_child_other_father:  list_child_other_father.remove(-1)\n",
    "                            #    list_child_other_father.remove(tmp_next_Points) # father all child's\n",
    "                            #    #print('list_child_other_father: ',list_child_other_father)\n",
    "                            #    list_childs = list(filter(lambda list_samecluster_full_childs: list_samecluster_full_childs[0] not in list_child_other_father, list_samecluster_full_childs))\n",
    "                            #    #print('list_childs ',idx_cluster_child,': ', list_childs)\n",
    "                            #    for x in list_childs: x[1]=elec_point\n",
    "                            #    #print('list_childs ',idx_cluster_child,' (update): ', list_childs)\n",
    "                            #-----------------\n",
    "        \n",
    "\n",
    "                            \n",
    "                    else: # N is one of initial point\n",
    "                        if (len(ls_clu_Pnt[tmp_next_Points])<3): #N subset initial_Point & befor not selected(dont have child)\n",
    "                            #print('initial_Point:', initial_Point)\n",
    "                            #if (len(ls_clu_Pnt[elec_point])==1):                \n",
    "                            #    ls_clu_Pnt[elec_point].insert(1,-1)\n",
    "                            #--find elected's father \n",
    "                            #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                            rdd = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                            if (len(rdd) >0):\n",
    "                                if rdd[0][1] == -1: \n",
    "                                    father_point= rdd[0][0]\n",
    "                                else:\n",
    "                                    father_point= rdd[0][1]\n",
    "                                if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                                    #ls_clu_Pnt[elec_point].append(father_point)  #(col1:point number , col2: father point, col3: N ...) \n",
    "                                    ls_clu_Pnt[elec_point].insert(1,father_point)\n",
    "                            #--\n",
    "                            ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                            ls_clu_Pnt[tmp_next_Points].remove(-1) \n",
    "                            initial_Point.remove(tmp_next_Points)\n",
    "                            elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                            elec_point_again= elec_point_again[0]\n",
    "                            #print('elec_point_again: ',elec_point_again)\n",
    "                            ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                            ls_sel_pnt.append(elec_point_again)\n",
    "                            initial_Point.append(elec_point_again)  \n",
    "                            #print('initial_Point new:', initial_Point)\n",
    "                        \n",
    "                        else: #((len(ls_clu_Pnt[tmp_next_Points])<3): #N subset initial_Point & befor..)\n",
    "                        #N(tmp_next_Points) subset initial_Point & befor selected(have child)\n",
    "                            #print('N(tmp_next_Points) subset initial_Point & befor selected(have child)')\n",
    "                            #---------------update childs(find childs & assign other father(elected)    \n",
    "                            #--find elected father \n",
    "                            #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                            rdd = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                            father_point=-100 # for know of wrong\n",
    "                            if (len(rdd) >0): # e sure have father\n",
    "                                if rdd[0][1] == -1: \n",
    "                                    father_point= rdd[0][0]\n",
    "                                else:\n",
    "                                    father_point= rdd[0][1]\n",
    "                                #if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                                    #ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                                #ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                            else: # e is elected\n",
    "                                #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[0:1])) ).collect() #find index father (in col 2 of list)\n",
    "                                #if (len(ls_clu_Pnt[elec_point])) ==2):# e is elected\n",
    "                                #    father_point=ls_clu_Pnt[tmp_next_Points][1]\n",
    "                                #else:\n",
    "                                    #father_point=ls_clu_Pnt[tmp_next_Points][1]\n",
    "                                father_point=elec_point\n",
    "                                    \n",
    "                            #--\n",
    "                            if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                                #--find list childs of childs tmp_next_Points & update fathers\n",
    "                                #print(\"father point:\",father_point)\n",
    "                                ls_clu_Pnt = rdd_cluster_pnt.map(lambda x: update_father_child(x,tmp_next_Points,father_point)).collect()#[1:2] searech all items with index[1] only\n",
    "\n",
    "                                #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                                #list_childs = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: ((len(x)>2) and (tmp_next_Points in x[1:2])) ).collect()#[1:2] searech all items with index[1] only\n",
    "                                ##print('list_childs of childs ',tmp_next_Points,': ', list_childs)\n",
    "                                ##print(\"father point:\",father_point)\n",
    "                                #if (len(list_childs) >0):\n",
    "                                #    for x in list_childs: \n",
    "                                #        #print(\"child:\",x[1])\n",
    "                                #        x[1]= father_point\n",
    "                                #        #print(\"update child father:\",x[1])\n",
    "                                #--\n",
    "                                #--remove (befor electe) and reasign new elected\n",
    "                                if (ls_clu_Pnt[tmp_next_Points][1]==-1):\n",
    "                                    initial_Point.remove(tmp_next_Points)\n",
    "                                    elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                                    elec_point_again= elec_point_again[0]\n",
    "                                    #print('elec_point_again: ',elec_point_again)\n",
    "                                    ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                                    ls_sel_pnt.append(elec_point_again)\n",
    "                                    initial_Point.append(elec_point_again)  \n",
    "                                    #print('initial_Point new:', initial_Point)                            \n",
    "                                #--\n",
    "                            if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                                ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                            ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                            if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                                ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                            #-----------------   \n",
    "                #else:   #(len(rdd) ==0)                 \n",
    "                    #print(\"elec_point & tmp_next_Points are in same cluster\")\n",
    "                    \n",
    "            else: # (elec_point not in ls_clu_Pnt[tmp_next_Points][2:]: #check (4,0) for (0,4))\n",
    "                #if (elec_point in ls_sel_pnt) and (len(ls_sel_pnt)==3):  #run 1 and two point are in initial_point\n",
    "                if (len(ls_sel_pnt)==3):  #run 1 and two point are in initial_point\n",
    "                    #print('elec_point & tmp_next_Points are in initial_Point for first run')\n",
    "                    ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                    ls_clu_Pnt[tmp_next_Points].remove(-1) \n",
    "                    initial_Point.remove(tmp_next_Points)\n",
    "                    elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                    elec_point_again= elec_point_again[0]\n",
    "                    #print('elec_point_again: ',elec_point_again)\n",
    "                    ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                    ls_sel_pnt.append(elec_point_again)\n",
    "                    initial_Point.append(elec_point_again)  \n",
    "                    #print('initial_Point new:', initial_Point)\n",
    "                #else: #((elec_point in ls_sel_pnt) and (len(ls_sel_pnt)==3):)\n",
    "                    #print('2,1 => 1,2 : n father e => e father n')\n",
    "        else: #(tmp_next_Points in ls_sel_pnt: #N befor selected)\n",
    "            #Asign_point= deep_index2(ls_clu_Pnt, tmp_next_Points) #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "            ##print(Asign_point) #=>[(7, 1)] (1->index 3)\n",
    "            #Asign_point=Asign_point[0][0]\n",
    "            ##print(Asign_point) #=> 7 (7 -> cluster dad(Asign))            \n",
    "            #print(\"tmp_next_Points without father & child\")\n",
    "            #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "            #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>1 and (x[0]==tmp_next_Points)  and (x[1]!=-1))).collect() #find index father (in col 2 of list)\n",
    "            #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "            rdd = rdd_cluster_pnt.map(lambda x: x).filter(lambda x: (len(x)>0 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "            if (len(rdd) >0):\n",
    "                if rdd[0][1] == -1: \n",
    "                    father_point= rdd[0][0]\n",
    "                else:\n",
    "                    father_point= rdd[0][1]\n",
    "                if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                    ls_clu_Pnt[elec_point].append(father_point)  #(col1:point number , col2: father point, col3: N ...) \n",
    "            else:\n",
    "                if (elec_point not in initial_Point):\n",
    "                    ls_clu_Pnt[elec_point].insert(1,-1)\n",
    "                    \n",
    "            ls_clu_Pnt[elec_point].append(tmp_next_Points)       \n",
    "            ls_sel_pnt.append(tmp_next_Points)\n",
    "            \n",
    "            \n",
    "\n",
    "        #print('ls_clu_Pnt: ',ls_clu_Pnt.__str__())\n",
    "        #print('ls_sel_pnt: ',ls_sel_pnt.__str__())\n",
    "        i=i+1\n",
    "            \n",
    "    \n",
    "    #print(\"-----------#print clusters-------------\")\n",
    "    rdd = sc.parallelize(ls_clu_Pnt)#find elect father (check for e & n are in samecluster(no assignment))\n",
    "    list_all_cluster = rdd.map(lambda x: x).filter(lambda x: (len(x)>1) and (x[1]==-1)).collect()\n",
    "    ##print(list_all_cluster7)\n",
    "    list_all_full_cluster=[]\n",
    "    if (len(list_all_cluster) >0): \n",
    "        for y in list_all_cluster: \n",
    "            ##print(y)\n",
    "            list_cluster=[]\n",
    "            list_cluster = rdd.map(lambda x: x).filter(lambda x: (len(x)>1) and (y[0]==x[1])).collect()\n",
    "            list_cluster.insert(0,y)\n",
    "            ##print(list_cluster)\n",
    "            for x in list_cluster: x.remove(x[1])\n",
    "            list_cluster= sum(list_cluster, []) #convert [[4,5],[3]] to [4,5,3]\n",
    "            ##print(list_cluster)\n",
    "            list_cluster = list( dict.fromkeys(list_cluster) )\n",
    "            ##print(list_cluster)\n",
    "            list_all_full_cluster.append(list_cluster)\n",
    "    \n",
    "    print(\"list_all_full_cluster:\",list_all_full_cluster)\n",
    "        \n",
    "\n",
    "#sc.stop()\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
